{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4e7ee2",
   "metadata": {},
   "source": [
    "# Tarefas abandonadas com PySpark\n",
    "\n",
    "Este projeto utiliza PySpark para identificar e analisar tarefas abandonadas ao longo do tempo, com o objetivo de monitorar padrões de abandono e apoiar a tomada de decisões para melhorias.\n",
    "\n",
    "**Fluxo do projeto:**\n",
    "\n",
    "1. **Extração e transformação:** Os dados são extraídos de um arquivo CSV, tratados e transformados em DataFrames Spark.\n",
    "2. **Persistência:** Os dados processados são enviados e armazenados em uma tabela DynamoDB.\n",
    "3. **Consulta e análise:** Os dados são recuperados do DynamoDB, processados em DataFrames e utilizados para gerar relatórios mensais dos últimos 6 meses, detalhando o volume de tarefas não concluídas por tipo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407851c8",
   "metadata": {},
   "source": [
    "## 1. Extração e transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f6af94-4c5d-40ab-92ca-a79de82a5922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/11 15:43:52 WARN Utils: Your hostname, kassia-pc, resolves to a loopback address: 127.0.1.1; using 172.18.227.129 instead (on interface eth0)\n",
      "25/06/11 15:43:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/11 15:43:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from src.clients.spark_client import get_spark_session\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, trim, when, lit,  current_timestamp, try_to_timestamp, date_format, concat, udf\n",
    "import uuid\n",
    "\n",
    "\n",
    "spark = get_spark_session()\n",
    "\n",
    "\n",
    "base_dataframe = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('delimiter', ';')\n",
    "    .option('header', 'true')\n",
    "    .option('inferSchema', 'true')\n",
    "    .option('enconding', 'ISO-8859-1')\n",
    "    .csv('./data/amostragem.csv')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df = base_dataframe.withColumnsRenamed(\n",
    "    {'Nome da Tarefa': 'nome', \n",
    "     'Data de Criação': 'data', \n",
    "     'Data de Conclusão': 'data_conclusao', \n",
    "     'Status': 'status', \n",
    "     'Tipo da Tarefa': 'tipo_tarefa', \n",
    "     'ID do Usuário': 'user_id',\n",
    "     'Usuário': 'usuario'\n",
    "     }\n",
    "    )\n",
    "\n",
    "df = df.withColumn('status', df['status'].cast(StringType()))\n",
    "df = df.withColumn('data', df['data'].cast(StringType()))\n",
    "df = df.withColumn('data_conclusao', df['data_conclusao'].cast(StringType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e2590",
   "metadata": {},
   "source": [
    "Realiza a limpeza dos dados conforme os critérios abaixo:\n",
    "- Exclui tarefas cujo nome está nulo ou vazio.\n",
    "- Atribui a data atual para tarefas sem data ou com data inválida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fcdd85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.filter((df['usuario'] == 'Jeferson Klau') & (df['status'] != '3'))\n",
    "df = df.distinct()\n",
    "df = df.withColumn('nome', trim('nome'))\n",
    "df = df.withColumn('nome', when(col('nome') == '', lit(None)).otherwise(col('nome')))\n",
    "df = df.dropna(subset=['nome'])\n",
    "\n",
    "df = df.withColumn('data', when(try_to_timestamp(col('data')).isNotNull(), col('data')).otherwise(current_timestamp().cast(StringType())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f957f45",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "Faz a transformação dos dados para o padrão esperado da tabela do DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "42026796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_uuid():\n",
    "  return str(uuid.uuid4())\n",
    "\n",
    "uuid_udf = udf(generate_uuid, StringType())\n",
    "user_id = '035c6ada-4091-703a-1837-677cad18d4a5'\n",
    "\n",
    "df = df.withColumn('status', when(df['status'] == '1', 'TODO').otherwise('DONE'))\n",
    "df = df.withColumn('user_id', when((df['user_id'] == 'b4853fc1f03a3a4cec530a98a94d89ad') | (df['usuario'] == 'Jeferson Klau'), user_id))\n",
    "df = df.withColumn('PK', concat(lit('LIST#'), date_format(col('data'), 'yyyyMMdd')))\n",
    "df = df.withColumn('SK', concat(lit('ITEM#'), uuid_udf()))  \n",
    "df = df.select(df.PK, df.SK, df.user_id, df.usuario, df.nome, df.data, df.data_conclusao, df.status, df.tipo_tarefa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6892c",
   "metadata": {},
   "source": [
    "## 2. Enviando os dados para a tabela do dynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c8a067d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos items foram inseridos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from src.clients.dynamodb_client import DynamoDBClient\n",
    "\n",
    "TAMANHO_LOTE = 100\n",
    "DELAY_EM_SEGUNDOS = 2\n",
    "\n",
    "\n",
    "dynamodb = DynamoDBClient()\n",
    "table = dynamodb.get_table('shopping-list')\n",
    "\n",
    "pandas_df = df.toPandas()\n",
    "rows = pandas_df.to_dict(orient='records')\n",
    "\n",
    "for i in range(0, len(rows), TAMANHO_LOTE):\n",
    "  batch_rows = rows[i:i+TAMANHO_LOTE]\n",
    "\n",
    "  with table.batch_writer() as batch:\n",
    "    for item in batch_rows:\n",
    "      \n",
    "      batch.put_item(Item=item) \n",
    "\n",
    "  time.sleep(DELAY_EM_SEGUNDOS)\n",
    "\n",
    "print('Todos items foram inseridos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6db75",
   "metadata": {},
   "source": [
    "## 3. Obtendo volume de tarefas abandonadas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8433dc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/12 09:36:22 WARN Utils: Your hostname, kassia-pc, resolves to a loopback address: 127.0.1.1; using 172.18.227.129 instead (on interface eth0)\n",
      "25/06/12 09:36:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/12 09:36:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from src.clients.spark_client import get_spark_session\n",
    "from src.clients.dynamodb_client import DynamoDBClient\n",
    "from boto3.dynamodb.conditions import Key\n",
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "\n",
    "spark = get_spark_session()\n",
    "\n",
    "# Query para obter as tarefas não concluídas\n",
    "dynamodb = DynamoDBClient()\n",
    "table = dynamodb.get_table('shopping-list')\n",
    "\n",
    "response = table.query(\n",
    "    IndexName='user_id',\n",
    "    KeyConditionExpression=Key('user_id').eq('035c6ada-4091-703a-1837-677cad18d4a5') & Key('PK').begins_with('LIST#'),\n",
    "    FilterExpression=Key('status').eq('TODO')\n",
    ")\n",
    "\n",
    "items = response['Items']\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('PK', StringType(), False),\n",
    "    StructField('SK', StringType(), False),\n",
    "    StructField('user_id', StringType(), False),\n",
    "    StructField('usuario', StringType(), False),\n",
    "    StructField('nome', StringType(), False),\n",
    "    StructField('data', StringType(), False),\n",
    "    StructField('data_conclusao', StringType(), True),\n",
    "    StructField('status', StringType(), False),\n",
    "    StructField('tipo_tarefa', StringType(), False)\n",
    "])\n",
    "\n",
    "dynamodb_dataframe = spark.createDataFrame(items, schema=schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "353e68e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "today = date.today()\n",
    "datas = [(today - relativedelta(months=i)).strftime('%Y-%m') for i in range(5, -1, -1)]\n",
    "\n",
    "abandonadas_df = (\n",
    "        dynamodb_dataframe\n",
    "        .select(\n",
    "            F.col('nome'),\n",
    "            F.col('data'),\n",
    "            F.col('tipo_tarefa')\n",
    "        \n",
    "        )\n",
    "        .where(\n",
    "            ((F.col('tipo_tarefa') == 'Tarefa a Ser Feita') & (F.datediff(F.lit(today), F.col('data')) > 15)) |\n",
    "            ((F.col('tipo_tarefa') == 'Item de Compra') & (F.datediff(F.lit(today), F.col('data')) > 30)) \n",
    "        )\n",
    "        .withColumn('mes', F.date_format(F.col('data'), 'yyyy-MM'))\n",
    "        .orderBy('tipo_tarefa')\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for data in datas:\n",
    "    df = abandonadas_df\\\n",
    "    .where(F.col('mes').like(data))\\\n",
    "    .groupby(F.col('tipo_tarefa'))\\\n",
    "    .count()\\\n",
    "    .select(F.col('tipo_tarefa'), F.col('count').alias(data))\\\n",
    "    .toPandas()\\\n",
    "    .set_index('tipo_tarefa')\n",
    "    resultados.append(df)\n",
    "    \n",
    "\n",
    "print(len(resultados))\n",
    "    \n",
    "\n",
    "abandonadas_por_data = pd.concat(resultados, axis=1)\n",
    "abandonadas_por_data.to_excel('abandonadas.xlsx', index_label=[\"\"], index=True, na_rep=0, sheet_name='Tarefas abandonadas por data')\n",
    "\n",
    "   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-spark-hv03Ohfi-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
